{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "from utils import FileIO\n",
    "file_io = FileIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionaire = file_io.load_json(\n",
    "    '../llm_baseline/prompts/envent_questionnaire_reader.json'\n",
    ")\n",
    "question_to_name = {\n",
    "    question['Dname']: question['Dquestion'] for question in questionaire\n",
    "}\n",
    "appraisal_d_lst = list(question_to_name.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_and_var_dist(fpath: str, model_name: str):\n",
    "    \n",
    "    model_results = json.load(open(fpath, 'r'))\n",
    "\n",
    "    model_mean_lst = []\n",
    "    model_dim_mean_lst = [[] for _ in range(21)]\n",
    "    human_mean_lst = []\n",
    "    human_dim_mean_lst = [[] for _ in range(21)]\n",
    "    model_var_lst = []\n",
    "    model_dim_var_lst = [[] for _ in range(21)]\n",
    "    human_var_lst = []\n",
    "    human_dim_var_lst = [[] for _ in range(21)]\n",
    "    for entry in model_results:\n",
    "        model_appraisal_d = entry['appraisal_d_pred_list']\n",
    "        human_appraisal_d = entry['appraisal_d_list']\n",
    "\n",
    "        human_appraisal_d = [ele['appraisal_d'] for ele in human_appraisal_d]\n",
    "\n",
    "        temp_model_app_lst = [[] for _ in range(21)]\n",
    "        temp_human_app_lst = [[] for _ in range(21)]\n",
    "        for i in range(5):\n",
    "\n",
    "            cur_model_appraisal_d = list(model_appraisal_d[i].values())\n",
    "            if len(cur_model_appraisal_d) != 21:\n",
    "                continue\n",
    "\n",
    "            for j in range(21):\n",
    "                temp_model_app_lst[j].append(int(cur_model_appraisal_d[j]))\n",
    "\n",
    "            cur_human_appraisal_d = list(human_appraisal_d[i].values())\n",
    "            for j in range(21):\n",
    "                temp_human_app_lst[j].append(int(cur_human_appraisal_d[j]))\n",
    "\n",
    "        temp_model_app_mean_lst = [np.mean(ele) for ele in temp_model_app_lst]\n",
    "        temp_human_app_mean_lst = [np.mean(ele) for ele in temp_human_app_lst]\n",
    "\n",
    "        for j in range(21):\n",
    "            model_dim_mean_lst[j].append(temp_model_app_mean_lst[j])\n",
    "            human_dim_mean_lst[j].append(temp_human_app_mean_lst[j])\n",
    "\n",
    "        temp_model_app_var_lst = [np.var(ele) for ele in temp_model_app_lst]\n",
    "        temp_human_app_var_lst = [np.var(ele) for ele in temp_human_app_lst]\n",
    "\n",
    "        for j in range(21):\n",
    "            model_dim_var_lst[j].append(temp_model_app_var_lst[j])\n",
    "            human_dim_var_lst[j].append(temp_human_app_var_lst[j])\n",
    "\n",
    "        model_mean_lst.append(temp_model_app_mean_lst)\n",
    "        human_mean_lst.append(temp_human_app_mean_lst)\n",
    "        model_var_lst.append(temp_model_app_var_lst)\n",
    "        human_var_lst.append(temp_human_app_var_lst)\n",
    "\n",
    "    unimodal_model_mean_lst = sum(model_mean_lst, [])\n",
    "    human_mean_lst = sum(human_mean_lst, [])\n",
    "\n",
    "    sns.kdeplot(unimodal_model_mean_lst, label=f'{model_name} Model')\n",
    "    sns.kdeplot(human_mean_lst, label='Human')\n",
    "    plt.title('Model vs Human Mean')\n",
    "    plt.xlabel('Mean')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./figures/{model_name}/var.png')\n",
    "    plt.close()\n",
    "\n",
    "    model_var_lst = sum(model_var_lst, [])\n",
    "    human_var_lst = sum(human_var_lst, [])\n",
    "    sns.kdeplot(model_var_lst, label=f'{model_name} Model')\n",
    "    sns.kdeplot(human_var_lst, label='Human')\n",
    "    plt.title('Model vs Human Variance')\n",
    "    plt.xlabel('Variance')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./figures/{model_name}/mean.png')\n",
    "    plt.close()\n",
    "\n",
    "    for i in range(21):\n",
    "        cur_dim_name = appraisal_d_lst[i]\n",
    "        sns.kdeplot(model_dim_mean_lst[i], label=f'Unimodal Model {cur_dim_name}')\n",
    "        sns.kdeplot(human_dim_mean_lst[i], label=f'Human {cur_dim_name}')\n",
    "        plt.title(f'{cur_dim_name} Mean')\n",
    "        plt.xlabel('Mean')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig(f'./figures/{model_name}/{cur_dim_name}_mean.png')\n",
    "        plt.close()\n",
    "\n",
    "    for i in range(21):\n",
    "        cur_dim_name = appraisal_d_lst[i]\n",
    "        sns.kdeplot(model_dim_var_lst[i], label=f'Unimodal Model {cur_dim_name}')\n",
    "        sns.kdeplot(human_dim_var_lst[i], label=f'Human {cur_dim_name}')\n",
    "        plt.title(f'{cur_dim_name} Variance')\n",
    "        plt.xlabel('Variance')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.savefig(f'./figures/{model_name}/{cur_dim_name}_var.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_mean_and_var_dist(\n",
    "#     fpath='./cache/deberta_large_unimodal_formatted.json',\n",
    "#     model_name='unimodal-deberta'\n",
    "# )\n",
    "# visualize_mean_and_var_dist(\n",
    "#     fpath='./cache/deberta_large_bimodal_formatted.json',\n",
    "#     model_name='bimodal-deberta'\n",
    "# )\n",
    "# visualize_mean_and_var_dist(\n",
    "#     fpath='../llm_baseline/cache/llama8_fomatted.json',\n",
    "#     model_name='llama'\n",
    "# )\n",
    "# visualize_mean_and_var_dist(\n",
    "#     fpath='../llm_baseline/cache/qwen7_fomatted.json',\n",
    "#     model_name='qwen'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Wasserstein distance w.r.t. appraisal dimension\n",
    "def rank_appraisal_dim_with_wasserstein(fpath: str):\n",
    "    model_results = json.load(open(fpath, 'r'))\n",
    "\n",
    "    appraisal_d_wasserstein_dict = {}\n",
    "    for entry in model_results:\n",
    "        predicted_appraisal_d = entry['appraisal_d_pred_list']\n",
    "        human_appraisal_d = entry['appraisal_d_list']\n",
    "\n",
    "        cur_predicted_appraisal_d = {}\n",
    "        cur_human_appraisal_d = {}\n",
    "        for i in range(5):\n",
    "            cur_prediction = predicted_appraisal_d[i]\n",
    "            cur_human = human_appraisal_d[i]['appraisal_d']\n",
    "\n",
    "            for name, rate in cur_prediction.items():\n",
    "                if name not in cur_predicted_appraisal_d:\n",
    "                    cur_predicted_appraisal_d[name] = []\n",
    "\n",
    "                cur_predicted_appraisal_d[name].append(rate)\n",
    "\n",
    "            for name, rate in cur_human.items():\n",
    "                if name not in cur_human_appraisal_d:\n",
    "                    cur_human_appraisal_d[name] = []\n",
    "\n",
    "                cur_human_appraisal_d[name].append(rate)\n",
    "\n",
    "        for (name, pred_rate), (_, human_rate) in zip(cur_predicted_appraisal_d.items(), cur_human_appraisal_d.items()):\n",
    "\n",
    "            if name not in appraisal_d_wasserstein_dict:\n",
    "                appraisal_d_wasserstein_dict[name] = []\n",
    "\n",
    "            cur_wasserstein_d = wasserstein_distance(pred_rate, human_rate)\n",
    "            appraisal_d_wasserstein_dict[name].append(cur_wasserstein_d)\n",
    "\n",
    "    appraisal_d_wasserstein_dict = {k: round(np.mean(v), 3) for k, v in appraisal_d_wasserstein_dict.items()}\n",
    "    appraisal_d_wasserstein_dict = {\n",
    "        k: tuple((v, idx)) for idx, (k, v) in enumerate(sorted(\n",
    "            appraisal_d_wasserstein_dict.items(), key=lambda item: item[1], reverse=False\n",
    "        ))\n",
    "    }\n",
    "    return appraisal_d_wasserstein_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_appraisal_d_wasserstein_dict = rank_appraisal_dim_with_wasserstein('./cache/deberta_large_unimodal_formatted.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_appraisal_d_wasserstein_dict = rank_appraisal_dim_with_wasserstein('./cache/deberta_large_bimodal_formatted.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_appraisal_d_wasserstein_dict = rank_appraisal_dim_with_wasserstein('../llm_baseline/cache/llama8_fomatted.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw_appraisal_d_wasserstein_dict = rank_appraisal_dim_with_wasserstein('../llm_baseline/cache/qwen7_fomatted.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll70_appraisal_d_wasserstein_dict = rank_appraisal_dim_with_wasserstein('../llm_baseline/cache/llama70_fomatted.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw72_appraisal_d_wasserstein_dict = rank_appraisal_dim_with_wasserstein('../llm_baseline/cache/qwen72_fomatted.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_appraisal_d_wasserstein_dict = rank_appraisal_dim_with_wasserstein('../vae-exp/vae/predictions/envent_test_reader_pred.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accept_conseq': (1.044, 0),\n",
       " 'predict_conseq': (1.059, 1),\n",
       " 'predict_event': (1.089, 2),\n",
       " 'effort': (1.11, 3),\n",
       " 'familiarity': (1.124, 4),\n",
       " 'urgency': (1.158, 5),\n",
       " 'goal_relevance': (1.17, 6),\n",
       " 'self_control': (1.187, 7),\n",
       " 'attention': (1.2, 8),\n",
       " 'suddenness': (1.252, 9),\n",
       " 'not_consider': (1.311, 10),\n",
       " 'chance_control': (1.34, 11),\n",
       " 'self_responsblt': (1.352, 12),\n",
       " 'goal_support': (1.36, 13),\n",
       " 'other_control': (1.362, 14),\n",
       " 'chance_responsblt': (1.363, 15),\n",
       " 'standards': (1.382, 16),\n",
       " 'other_responsblt': (1.389, 17),\n",
       " 'unpleasantness': (1.524, 18),\n",
       " 'social_norms': (1.533, 19),\n",
       " 'pleasantness': (1.558, 20)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_appraisal_d_wasserstein_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_ranking_dict = {}\n",
    "\n",
    "for name, tup in ud_appraisal_d_wasserstein_dict.items():\n",
    "    bd_tup = bd_appraisal_d_wasserstein_dict[name]\n",
    "    ll_tup = ll_appraisal_d_wasserstein_dict[name]\n",
    "    qw_tup = qw_appraisal_d_wasserstein_dict[name]\n",
    "\n",
    "    aggregated_ranking_dict[name] = round(np.mean([tup[1], bd_tup[1], ll_tup[1], qw_tup[1]]), 3)\n",
    "\n",
    "aggregated_ranking_dict = {\n",
    "    k: v for k, v in sorted(\n",
    "        aggregated_ranking_dict.items(), key=lambda item: item[1]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pleasantness': 5.0,\n",
       " 'unpleasantness': 5.25,\n",
       " 'social_norms': 5.5,\n",
       " 'chance_responsblt': 6.25,\n",
       " 'suddenness': 6.5,\n",
       " 'goal_relevance': 7.0,\n",
       " 'standards': 7.25,\n",
       " 'effort': 7.5,\n",
       " 'attention': 7.75,\n",
       " 'chance_control': 8.0,\n",
       " 'self_control': 8.25,\n",
       " 'self_responsblt': 9.25,\n",
       " 'familiarity': 10.75,\n",
       " 'other_responsblt': 12.25,\n",
       " 'predict_event': 12.75,\n",
       " 'predict_conseq': 13.0,\n",
       " 'accept_conseq': 13.5,\n",
       " 'not_consider': 14.0,\n",
       " 'urgency': 14.25,\n",
       " 'other_control': 16.5,\n",
       " 'goal_support': 19.5}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_ranking_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masktom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
